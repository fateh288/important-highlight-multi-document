{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import scipy.cluster.hierarchy as hcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# topics = [\"ml1\", \"ml2\", \"ml3\", \"ml4\", \"ml5\", \"ml6\", \"ml7\", \"ml8\", \"ml9\", \"ml10\", \"ml11\", \"ml12\", \"ml13\", \"ml14\", \"ml15\", \"ml16\", \"ml17\", \"ml18\", \"ml19\"]\n",
    "# print \"The titles are \", topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "filePath = './books'\n",
    "fileCounter = len(glob.glob(filePath + \"*.txt\"))\n",
    "\n",
    "paragraphs = []\n",
    "topics = []\n",
    "for index, filename in enumerate(glob.glob(filePath + '/*.txt')):\n",
    "    print(filename)\n",
    "    file = open(filename, 'r')\n",
    "    txt = file.read()\n",
    "    txt = txt.replace('\\n', '').split('.')\n",
    "    txt = filter(None, txt)\n",
    "    print len(txt)\n",
    "    for i in np.arange(0, len(txt)):\n",
    "        topics.append('book:%d sentence:%d' % (index, i))\n",
    "        paragraphs.append(txt[i])\n",
    "\n",
    "print len(topics)\n",
    "print len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now we want to break a word into its root using a stemmer.\n",
    "#We use the snowball.\n",
    "#snowball is better than porter stemmer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#First the parageraph is tokenized by sentence then by word\n",
    "def tokenize(paragraph) :\n",
    "    tokenList = [word for sentence in nltk.sent_tokenize(paragraph) for word in nltk.word_tokenize(sentence)]\n",
    "    \n",
    "    filteredTokens = []\n",
    "    \n",
    "    for token in tokenList:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filteredTokens.append(token)\n",
    "    return filteredTokens\n",
    "\n",
    "#Next step would be to remove the useless tokens. Useless tokens are raw puntuation, numeric tokens, etc.\n",
    "def stem(filteredTokens) :\n",
    "    stemList = [stemmer.stem(tok) for tok in filteredTokens]\n",
    "    return stemList\n",
    "\n",
    "def tokenizeAndStem(paragraph) :\n",
    "    tokens = [word for sentence in nltk.sent_tokenize(paragraph) for word in nltk.word_tokenize(sentence)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):  #normal regex search\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "tokenizedParagraphList = []\n",
    "stemmedParagraphList = []\n",
    "for i in paragraphs :\n",
    "    tokenizedParagraph = tokenize(i)\n",
    "    tokenizedParagraphList.extend(tokenizedParagraph)\n",
    "    stemmedParagraphList.extend(stem(tokenizedParagraph))\n",
    "print tokenizedParagraphList\n",
    "print stemmedParagraphList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column.\n",
    "#The benefit of this is it provides an efficient way to look up a stem and return a full token. \n",
    "#The downside here is that stems to tokens are one to many: the stem 'run' could be associated with 'ran', 'runs', 'running', etc.\n",
    "\n",
    "\n",
    "vocabFrame = pd.DataFrame({'words': tokenizedParagraphList}, index = stemmedParagraphList)\n",
    "print 'there are ' + str(vocabFrame.shape[0]) + ' items in vocab_frame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for tfidf, there are two important things, max_df & min_df;\n",
    "#max_df - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "#min_idf - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVectorizer = TfidfVectorizer(stop_words='english', use_idf=True, tokenizer=tokenizeAndStem, ngram_range=(1,3))\n",
    "#Now fit the vectorizer to synopses\n",
    "%time tfidf_matrix = tfidfVectorizer.fit_transform(paragraphs)\n",
    "print tfidf_matrix\n",
    "print tfidfVectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we calculate the cosine similarity as follows. This will gives us the distance which wil help us in clustering in the later stage.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresh=1.05*np.average(dist)\n",
    "clusters = hcluster.fclusterdata(dist, thresh, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clusters)\n",
    "numClusters = max(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterStatements = []\n",
    "for i in range(0,numClusters,1) :\n",
    "    clusterStatements.append([])\n",
    "for i in range(0, len(paragraphs), 1) :\n",
    "    clusterStatements[clusters[i]-1].append(paragraphs[i])\n",
    "    #tempClusterStatements[clusters[i]] += paragraphs[i]\n",
    "print clusterStatements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in range(0, numClusters, 1) :\n",
    "    tfidfVectorizerCluster = TfidfVectorizer(stop_words='english', use_idf=True, tokenizer=tokenizeAndStem, ngram_range=(1,3))\n",
    "    tfidfMatrixCluster = tfidfVectorizerCluster.fit_transform(clusterStatements[k])\n",
    "    distCluster = 1 - cosine_similarity(tfidfMatrixCluster)\n",
    "    maxClusterDistance=0\n",
    "    farthestSentence=0\n",
    "    for i in range(0, len(distCluster), 1) :\n",
    "        temp=0\n",
    "        for j in range(0, len(distCluster[i]), 1) :\n",
    "            temp+=distCluster[i][j]\n",
    "        farthestSentence = farthestSentence if maxClusterDistance<temp else i\n",
    "        maxClusterDistance = maxClusterDistance if maxClusterDistance<temp else temp\n",
    "    print clusterStatements[k][farthestSentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}