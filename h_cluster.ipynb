{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3\n",
    "import scipy.cluster.hierarchy as hcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# topics = [\"ml1\", \"ml2\", \"ml3\", \"ml4\", \"ml5\", \"ml6\", \"ml7\", \"ml8\", \"ml9\", \"ml10\", \"ml11\", \"ml12\", \"ml13\", \"ml14\", \"ml15\", \"ml16\", \"ml17\", \"ml18\", \"ml19\"]\n",
    "# print \"The titles are \", topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./books/d3.txt\n96\n./books/d1.txt\n56\n./books/d2.txt\n21\n173\n173\n[[0, 0], [0, 1], [0, 2], [0, 3], [0, 4], [0, 5], [0, 6], [0, 7], [0, 8], [0, 9], [0, 10], [0, 11], [0, 12], [0, 13], [0, 14], [0, 15], [0, 16], [0, 17], [0, 18], [0, 19], [0, 20], [0, 21], [0, 22], [0, 23], [0, 24], [0, 25], [0, 26], [0, 27], [0, 28], [0, 29], [0, 30], [0, 31], [0, 32], [0, 33], [0, 34], [0, 35], [0, 36], [0, 37], [0, 38], [0, 39], [0, 40], [0, 41], [0, 42], [0, 43], [0, 44], [0, 45], [0, 46], [0, 47], [0, 48], [0, 49], [0, 50], [0, 51], [0, 52], [0, 53], [0, 54], [0, 55], [0, 56], [0, 57], [0, 58], [0, 59], [0, 60], [0, 61], [0, 62], [0, 63], [0, 64], [0, 65], [0, 66], [0, 67], [0, 68], [0, 69], [0, 70], [0, 71], [0, 72], [0, 73], [0, 74], [0, 75], [0, 76], [0, 77], [0, 78], [0, 79], [0, 80], [0, 81], [0, 82], [0, 83], [0, 84], [0, 85], [0, 86], [0, 87], [0, 88], [0, 89], [0, 90], [0, 91], [0, 92], [0, 93], [0, 94], [0, 95], [1, 0], [1, 1], [1, 2], [1, 3], [1, 4], [1, 5], [1, 6], [1, 7], [1, 8], [1, 9], [1, 10], [1, 11], [1, 12], [1, 13], [1, 14], [1, 15], [1, 16], [1, 17], [1, 18], [1, 19], [1, 20], [1, 21], [1, 22], [1, 23], [1, 24], [1, 25], [1, 26], [1, 27], [1, 28], [1, 29], [1, 30], [1, 31], [1, 32], [1, 33], [1, 34], [1, 35], [1, 36], [1, 37], [1, 38], [1, 39], [1, 40], [1, 41], [1, 42], [1, 43], [1, 44], [1, 45], [1, 46], [1, 47], [1, 48], [1, 49], [1, 50], [1, 51], [1, 52], [1, 53], [1, 54], [1, 55], [2, 0], [2, 1], [2, 2], [2, 3], [2, 4], [2, 5], [2, 6], [2, 7], [2, 8], [2, 9], [2, 10], [2, 11], [2, 12], [2, 13], [2, 14], [2, 15], [2, 16], [2, 17], [2, 18], [2, 19], [2, 20]]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "filePath = './books'\n",
    "fileCounter = len(glob.glob(filePath + \"*.txt\"))\n",
    "\n",
    "paragraphs = []\n",
    "topics = []\n",
    "book_sentence_pairs=[]\n",
    "for index, filename in enumerate(glob.glob(filePath + '/*.txt')):\n",
    "    print(filename)\n",
    "    file = open(filename, 'r')\n",
    "    txt = file.read()\n",
    "    txt = txt.replace('\\n', '').split('.')\n",
    "    txt = filter(None, txt)\n",
    "    print len(txt)\n",
    "    for i in np.arange(0, len(txt)):\n",
    "        topics.append('book: %d sentence: %d' % (index, i))\n",
    "        book_sentence_pairs.append([index,i])\n",
    "        paragraphs.append(txt[i])\n",
    "\n",
    "print len(topics)\n",
    "print len(paragraphs)\n",
    "print book_sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "print stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Now we want to break a word into its root using a stemmer.\n",
    "#We use the snowball.\n",
    "#snowball is better than porter stemmer\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#First the parageraph is tokenized by sentence then by word\n",
    "def tokenize(paragraph) :\n",
    "    tokenList = [word for sentence in nltk.sent_tokenize(paragraph) for word in nltk.word_tokenize(sentence)]\n",
    "    \n",
    "    filteredTokens = []\n",
    "    \n",
    "    for token in tokenList:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filteredTokens.append(token)\n",
    "    return filteredTokens\n",
    "\n",
    "#Next step would be to remove the useless tokens. Useless tokens are raw puntuation, numeric tokens, etc.\n",
    "def stem(filteredTokens) :\n",
    "    stemList = [stemmer.stem(tok) for tok in filteredTokens]\n",
    "    return stemList\n",
    "\n",
    "def tokenizeAndStem(paragraph) :\n",
    "    tokens = [word for sentence in nltk.sent_tokenize(paragraph) for word in nltk.word_tokenize(sentence)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):  #normal regex search\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "tokenizedParagraphList = []\n",
    "stemmedParagraphList = []\n",
    "for i in paragraphs :\n",
    "    tokenizedParagraph = tokenize(i)\n",
    "    tokenizedParagraphList.extend(tokenizedParagraph)\n",
    "    stemmedParagraphList.extend(stem(tokenizedParagraph))\n",
    "print tokenizedParagraphList\n",
    "print stemmedParagraphList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a pandas DataFrame with the stemmed vocabulary as the index and the tokenized words as the column.\n",
    "#The benefit of this is it provides an efficient way to look up a stem and return a full token. \n",
    "#The downside here is that stems to tokens are one to many: the stem 'run' could be associated with 'ran', 'runs', 'running', etc.\n",
    "\n",
    "\n",
    "vocabFrame = pd.DataFrame({'words': tokenizedParagraphList}, index = stemmedParagraphList)\n",
    "print 'there are ' + str(vocabFrame.shape[0]) + ' items in vocab_frame'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for tfidf, there are two important things, max_df & min_df;\n",
    "#max_df - When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "#min_idf - When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature.\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfVectorizer = TfidfVectorizer(stop_words='english', use_idf=True, tokenizer=tokenizeAndStem, ngram_range=(1,3))\n",
    "#Now fit the vectorizer to synopses\n",
    "%time tfidf_matrix = tfidfVectorizer.fit_transform(paragraphs)\n",
    "print tfidf_matrix\n",
    "print tfidfVectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we calculate the cosine similarity as follows. This will gives us the distance which wil help us in clustering in the later stage.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "print dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thresh=1.05*np.average(dist)\n",
    "clusters = hcluster.fclusterdata(dist, thresh, criterion=\"distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(clusters)\n",
    "numClusters = max(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clusterStatements = []\n",
    "clusterIndices = []\n",
    "for i in range(0,numClusters,1) :\n",
    "    clusterStatements.append([])\n",
    "    clusterIndices.append([])\n",
    "for i in range(0, len(paragraphs), 1) :\n",
    "    clusterStatements[clusters[i]-1].append(paragraphs[i])\n",
    "    clusterIndices[clusters[i]-1].append(i)\n",
    "    #tempClusterStatements[clusters[i]] += paragraphs[i]\n",
    "print clusterStatements\n",
    "print clusterIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_sentence_indices=[]\n",
    "for k in range(0, numClusters, 1) :\n",
    "    tfidfVectorizerCluster = TfidfVectorizer(stop_words='english', use_idf=True, tokenizer=tokenizeAndStem, ngram_range=(1,3))\n",
    "    tfidfMatrixCluster = tfidfVectorizerCluster.fit_transform(clusterStatements[k])\n",
    "    distCluster = 1 - cosine_similarity(tfidfMatrixCluster)\n",
    "    maxClusterDistance=0\n",
    "    farthestSentence=0\n",
    "    for i in range(0, len(distCluster), 1) :\n",
    "        temp=0\n",
    "        for j in range(0, len(distCluster[i]), 1) :\n",
    "            temp+=distCluster[i][j]\n",
    "        farthestSentence = farthestSentence if maxClusterDistance<temp else i\n",
    "        maxClusterDistance = maxClusterDistance if maxClusterDistance<temp else temp\n",
    "    final_sentence_indices.append(clusterIndices[k][farthestSentence])\n",
    "    print clusterIndices[k][farthestSentence]\n",
    "\n",
    "print final_sentence_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "final_book_sentence_pairs=[]\n",
    "for i in final_sentence_indices:\n",
    "    final_book_sentence_pairs.append(book_sentence_pairs[i])\n",
    "final_book_sentence_pairs=sorted(final_book_sentence_pairs,key=itemgetter(0,1))\n",
    "#sorted(final_book_sentence_pairs,key=itemgetter(1))\n",
    "print\n",
    "print\n",
    "print final_book_sentence_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdfkit\n",
    "\n",
    "for index, filename in enumerate(glob.glob(filePath + '/*.txt')):\n",
    "    file = open(filename, 'r')\n",
    "    txt = file.read()\n",
    "    txt = txt.replace('\\n', '').split('.')\n",
    "    for i,line in enumerate(final_book_sentence_pairs):\n",
    "        if(final_book_sentence_pairs[i][0]==index):\n",
    "            txt[line[1]]='<mark>'+txt[line[1]]+'</mark>'\n",
    "    txt='.'.join(txt)\n",
    "    Html_file= open(filename+'.html',\"w\")\n",
    "    Html_file.write(txt)\n",
    "    Html_file.close()\n",
    "    pdfkit.from_url(filename+'.html', filename+'.pdf')\n",
    "    #syscall=\"weasyprint %s %s\"%(os.path.splitext(filename)[0]+'.html', os.path.splitext(filename)[0]+'_marked.pdf')\n",
    "    #weasyprint mypage.html out.pdf\n",
    "    #print syscall\n",
    "    #os.system(syscall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}